{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxRuCjCFTD5z"
      },
      "outputs": [],
      "source": [
        "# Install Libraries\n",
        "\n",
        "!pip install -U transformers\n",
        "!pip install -q transformers datasets torch scikit-learn faiss-cpu accelerate peft bitsandbytes sentence-transformers\n",
        "!pip install -U transformers datasets accelerate --quiet\n",
        "!pip install streamlit pyngrok\n",
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import all necessary libraries\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "import faiss\n",
        "import streamlit as st\n",
        "import torch\n",
        "import pickle\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pyngrok import ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "roT0zARDWHNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Upload the dataset\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "PMuaqRPfWWws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the dataset\n",
        "\n",
        "df = pd.read_csv(\"bbc_news_text_complexity_summarization.csv\", delimiter=',')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xadrKpijWeoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YA2SqRx2V5dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "test_ds = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "wc4J_0wFWlFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df[\"labels\"].unique().tolist()\n",
        "print(\"Labels:\", labels)"
      ],
      "metadata": {
        "id": "oACMC7VWb3VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare FAISS Retriever (for RAG)\n",
        "\n",
        "corpus = df[\"text\"].tolist()\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus_embeddings = embed_model.encode(corpus, convert_to_numpy=True)\n",
        "index = faiss.IndexFlatL2(corpus_embeddings.shape[1])\n",
        "index.add(corpus_embeddings)"
      ],
      "metadata": {
        "id": "67Idz3UUWq3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Flan-T5 + LoRA\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "# Tokenizer (must exist before Trainer)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "EQ7JckmLW1iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Prompts (RAG)\n",
        "\n",
        "def retrieve_context(text, top_k=3):\n",
        "    query_emb = embed_model.encode([text], convert_to_numpy=True)\n",
        "    D, I = index.search(query_emb, top_k)\n",
        "    return \"\\n\".join([corpus[i] for i in I[0]])\n",
        "\n",
        "def format_rag_prompt(example):\n",
        "    context = retrieve_context(example[\"text\"])\n",
        "    example[\"prompt\"] = f\"\"\"Using the context below, classify the complexity of the following text as one of {labels}.\n",
        "\n",
        "Text:\n",
        "{example['text']}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    example[\"label_text\"] = example[\"labels\"]\n",
        "    return example\n",
        "\n",
        "train_ds = Dataset.from_pandas(df).map(format_rag_prompt)\n",
        "test_ds = Dataset.from_pandas(df).map(format_rag_prompt)\n"
      ],
      "metadata": {
        "id": "Qg3gX5-LavgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "\n",
        "max_length = 256\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"prompt\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "tokenized_train = train_ds.map(tokenize, batched=True)\n",
        "tokenized_test = test_ds.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "7_SO6dPma0wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collator\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "sEBiqAxaa3Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Arguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./flan-t5-bbc-rag-lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    predict_with_generate=True\n",
        ")"
      ],
      "metadata": {
        "id": "aybW5pNza8n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "zEza4GVCbI4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model in evaluation mode\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "wMXsUfl8bMIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG prediction function\n",
        "\n",
        "def predict_label_rag(text):\n",
        "    context = retrieve_context(text, top_k=3)\n",
        "\n",
        "    prompt = f\"\"\"Using the context below, classify the complexity of the following text as one of {labels}.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():  # VERY important for Colab\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10\n",
        "        )\n",
        "\n",
        "    pred_text = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True\n",
        "    ).lower()\n",
        "\n",
        "    for lbl in labels:\n",
        "        if lbl.lower() in pred_text:\n",
        "            return lbl\n",
        "\n",
        "    return \"unknown\""
      ],
      "metadata": {
        "id": "-zauab0IpZzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data\n",
        "\n",
        "test_df = df.sample(frac=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "rTDx2jrspj5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run predictions\n",
        "\n",
        "y_true = test_df[\"labels\"].tolist()\n",
        "y_pred = [predict_label_rag(x) for x in test_df[\"text\"]]"
      ],
      "metadata": {
        "id": "claGjvAfppts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the RAG + LoRA model on the test set.\n",
        "# Computes standard classification metrics:\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred,\n",
        "    average=\"macro\",\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(f\"RAG + LoRA Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (macro): {precision:.4f}\")\n",
        "print(f\"Recall (macro): {recall:.4f}\")\n",
        "print(f\"F1 (macro): {f1:.4f}\")"
      ],
      "metadata": {
        "id": "pn_xA_30p89E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA model\n",
        "model.save_pretrained(\"flan_t5_lora\")\n",
        "tokenizer.save_pretrained(\"flan_t5_lora\")\n",
        "\n",
        "# Save FAISS index\n",
        "faiss.write_index(index, \"faiss_index.bin\")\n",
        "\n",
        "# Save corpus\n",
        "import pickle\n",
        "with open(\"corpus.pkl\", \"wb\") as f:\n",
        "    pickle.dump(corpus, f)"
      ],
      "metadata": {
        "id": "HgPVfuCRZw8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment: Streamlit Web App with RAG + LoRA**\n",
        ": This section creates an interactive Streamlit application for real-time inference using the fine-tuned Flan-T5 model with LoRA and a Retrieval-Augmented Generation (RAG) pipeline."
      ],
      "metadata": {
        "id": "XZTuM6hexTh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit Web Application: RAG + LoRA Text Complexity Classifier\n",
        "\n",
        "%%writefile app.py\n",
        "\n",
        "st.set_page_config(page_title=\"RAG LLM Classifier\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_all():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"flan_t5_lora\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"flan_t5_lora\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    index = faiss.read_index(\"faiss_index.bin\")\n",
        "\n",
        "    with open(\"corpus.pkl\", \"rb\") as f:\n",
        "        corpus = pickle.load(f)\n",
        "\n",
        "    return tokenizer, model, embed_model, index, corpus, device\n",
        "\n",
        "tokenizer, model, embed_model, index, corpus, device = load_all()\n",
        "\n",
        "labels = [\"easy\", \"medium\", \"hard\"]\n",
        "\n",
        "def retrieve_context(text, top_k=3):\n",
        "    emb = embed_model.encode([text])\n",
        "    _, I = index.search(emb, top_k)\n",
        "    return \"\\n\".join([corpus[i] for i in I[0]])\n",
        "\n",
        "def predict(text):\n",
        "    context = retrieve_context(text)\n",
        "    prompt = f\"\"\"\n",
        "Using the context below, classify the text into exactly one label from {labels}.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer with exactly one label:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=5)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return prediction, context\n",
        "\n",
        "st.title(\"RAG-powered Text Complexity Classifier\")\n",
        "\n",
        "user_text = st.text_area(\"Enter text\")\n",
        "\n",
        "if st.button(\"Classify\"):\n",
        "    if user_text.strip():\n",
        "        label, ctx = predict(user_text)\n",
        "        st.success(f\"Prediction: {label}\")\n",
        "        st.subheader(\"Retrieved Context\")\n",
        "        st.write(ctx)\n",
        "    else:\n",
        "        st.warning(\"Please enter text\")"
      ],
      "metadata": {
        "id": "hzYNjNmcc44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expose the app (ngrok)\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "ZNtpJ-VndEnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add authtoken in Colab\n",
        "\n",
        "!ngrok config add-authtoken 396eACYUVuDbWHPwHhf6MsR9AZK_4jtq4swAgtirbVyJjYjuX"
      ],
      "metadata": {
        "id": "r9CvQXwGcfI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}